{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "254f2253",
   "metadata": {},
   "source": [
    "# Inference with Furiosa-LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b2473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 170 files: 100%|██████████| 170/170 [00:00<00:00, 5983.82it/s]\n",
      "INFO:2025-06-04 02:17:38+0000 Prefill buckets with output size: [BucketWithOutputLogitsSize(bucket=Bucket(batch_size=1, attention_size=256, kv_cache_size=0),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=1, attention_size=320, kv_cache_size=0),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=1, attention_size=384, kv_cache_size=0),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=1, attention_size=512, kv_cache_size=0),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=1, attention_size=640, kv_cache_size=0),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=1, attention_size=768, kv_cache_size=0),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=1, attention_size=1024, kv_cache_size=0),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=1, attention_size=4096, kv_cache_size=0),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=1, attention_size=8192, kv_cache_size=0),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=1, attention_size=10240, kv_cache_size=0),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=1, attention_size=8192, kv_cache_size=0),\n",
      "                            output_logits_size=0)]\n",
      "INFO:2025-06-04 02:17:38+0000 Decode buckets with output size: [BucketWithOutputLogitsSize(bucket=Bucket(batch_size=1, attention_size=1024, kv_cache_size=1023),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=2, attention_size=1024, kv_cache_size=1023),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=4, attention_size=1024, kv_cache_size=1023),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=8, attention_size=1024, kv_cache_size=1023),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=16, attention_size=1024, kv_cache_size=1023),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=32, attention_size=1024, kv_cache_size=1023),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=64, attention_size=1024, kv_cache_size=1023),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=128, attention_size=1024, kv_cache_size=1023),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=256, attention_size=1024, kv_cache_size=1023),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=2, attention_size=2048, kv_cache_size=2047),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=4, attention_size=2048, kv_cache_size=2047),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=8, attention_size=2048, kv_cache_size=2047),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=16, attention_size=2048, kv_cache_size=2047),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=32, attention_size=2048, kv_cache_size=2047),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=64, attention_size=2048, kv_cache_size=2047),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=128, attention_size=2048, kv_cache_size=2047),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=1, attention_size=11264, kv_cache_size=11263),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=2, attention_size=11264, kv_cache_size=11263),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=4, attention_size=11264, kv_cache_size=11263),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=8, attention_size=11264, kv_cache_size=11263),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=16, attention_size=11264, kv_cache_size=11263),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=32, attention_size=11264, kv_cache_size=11263),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=1, attention_size=32768, kv_cache_size=32767),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=2, attention_size=32768, kv_cache_size=32767),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=4, attention_size=32768, kv_cache_size=32767),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=8, attention_size=32768, kv_cache_size=32767),\n",
      "                            output_logits_size=1)]\n",
      "INFO:2025-06-04 02:17:38+0000 Other buckets with output size: [BucketWithOutputLogitsSize(bucket=Bucket(batch_size=1, attention_size=16384, kv_cache_size=8192),\n",
      "                            output_logits_size=0),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=1, attention_size=16384, kv_cache_size=8192),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=1, attention_size=24576, kv_cache_size=16384),\n",
      "                            output_logits_size=0),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=1, attention_size=24576, kv_cache_size=16384),\n",
      "                            output_logits_size=1),\n",
      " BucketWithOutputLogitsSize(bucket=Bucket(batch_size=1, attention_size=32768, kv_cache_size=24576),\n",
      "                            output_logits_size=1)]\n",
      "INFO:2025-06-04 02:17:38+0000 Device Mesh currently working is [['npu:0:0-7']] with tp_size=8/pp_size=1/dp_size=1\n",
      "INFO:2025-06-04 02:17:38+0000 For some LLaMA V1 models, initializing the fast tokenizer may take a long time. To reduce the initialization time, consider using 'hf-internal-testing/llama-tokenizer' instead of the original tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-06-04T02:17:38.996881029Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::backend::furiosa_rt\u001b[0m\u001b[2m:\u001b[0m Trying to open:\n",
      "DeviceRow([Device::npu_fused(0, 0..=7)])\n",
      "\u001b[2m2025-06-04T02:17:39.405244807Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::backend\u001b[0m\u001b[2m:\u001b[0m KV caches on for each layer (I8, total 8.4 MB * num_blocks over 32 layers)  will be allocated\n",
      "\u001b[2m2025-06-04T02:17:39.426210269Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::backend::furiosa_rt\u001b[0m\u001b[2m:\u001b[0m Loading 18955 parameters from storages has started ...\n",
      "\u001b[2m2025-06-04T02:17:39.426733339Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_sprinter::buffer::alloc\u001b[0m\u001b[2m:\u001b[0m Support for huge page size of 2 MiB has been detected.\n",
      "\u001b[2m2025-06-04T02:17:55.30925103Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::backend::furiosa_rt\u001b[0m\u001b[2m:\u001b[0m 18955 parameters (12.8 GiB) has been successfully loaded (15 secs).\n",
      "\u001b[2m2025-06-04T02:17:55.709550566Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::backend\u001b[0m\u001b[2m:\u001b[0m Determine the maximized available num_block as 424338\n",
      "\u001b[2m2025-06-04T02:17:56.986897218Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m Preparing Backend (DeviceIndex(0): npu:0):\n",
      "\u001b[2m2025-06-04T02:17:56.986922518Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 0] prefill batch: 1, attn_size: 256, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986925538Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 1] prefill batch: 1, attn_size: 320, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986927148Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 2] prefill batch: 1, attn_size: 384, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986928638Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 3] prefill batch: 1, attn_size: 512, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986930098Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 4] prefill batch: 1, attn_size: 640, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986931598Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 5] prefill batch: 1, attn_size: 768, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986933048Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 6] prefill batch: 1, attn_size: 1024, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986934578Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 7] prefill batch: 1, attn_size: 4096, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986936048Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 8] prefill batch: 1, attn_size: 8192, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986939128Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 9] prefill batch: 1, attn_size: 10240, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986940768Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 10] decode batch: 1, attn_size: 1024, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986942268Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 11] decode batch: 1, attn_size: 11264, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986943768Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 12] decode batch: 1, attn_size: 16384, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986945288Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 13] decode batch: 1, attn_size: 24576, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986946798Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 14] decode batch: 1, attn_size: 32768, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986948308Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 15] decode batch: 1, attn_size: 32768, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986949838Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 16] decode batch: 2, attn_size: 1024, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986951348Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 17] decode batch: 2, attn_size: 2048, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986952868Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 18] decode batch: 2, attn_size: 11264, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986958728Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 19] decode batch: 2, attn_size: 32768, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986960218Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 20] decode batch: 4, attn_size: 1024, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986961698Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 21] decode batch: 4, attn_size: 2048, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986963168Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 22] decode batch: 4, attn_size: 11264, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986964618Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 23] decode batch: 4, attn_size: 32768, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986966058Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 24] decode batch: 8, attn_size: 1024, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986967558Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 25] decode batch: 8, attn_size: 2048, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986969078Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 26] decode batch: 8, attn_size: 11264, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986970598Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 27] decode batch: 8, attn_size: 32768, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986972088Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 28] decode batch: 16, attn_size: 1024, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986973618Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 29] decode batch: 16, attn_size: 2048, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986975308Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 30] decode batch: 16, attn_size: 11264, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986976878Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 31] decode batch: 32, attn_size: 1024, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986978388Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 32] decode batch: 32, attn_size: 2048, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986979888Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 33] decode batch: 32, attn_size: 11264, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986981408Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 34] decode batch: 64, attn_size: 1024, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986982918Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 35] decode batch: 64, attn_size: 2048, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986984368Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 36] decode batch: 128, attn_size: 1024, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986985848Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 37] decode batch: 128, attn_size: 2048, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.986988108Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::action_provider\u001b[0m\u001b[2m:\u001b[0m  - [Pipeline 38] decode batch: 256, attn_size: 1024, 1 segments per device (total: 1 devices)\n",
      "\u001b[2m2025-06-04T02:17:56.987544938Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::generator\u001b[0m\u001b[2m:\u001b[0m Starting scheduler loop with config: SchedulerConfig { npu_queue_limit: 2, max_processing_samples: 65536, spare_blocks_ratio: 0.0, is_offline: false, prefill_chunk_size: Some(8192) }\n"
     ]
    }
   ],
   "source": [
    "from furiosa_llm import LLM, SamplingParams\n",
    "\n",
    "# Load the Llama 3.1 8B Instruct model\n",
    "llm = LLM.load_artifact(\"furiosa-ai/Llama-3.1-8B-Instruct-FP8\", devices=\"npu:0\")\n",
    "\n",
    "# You can specify various parameters for text generation\n",
    "sampling_params = SamplingParams(max_tokens=100, top_p=0.3, top_k=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab68a9f",
   "metadata": {},
   "source": [
    "### 1. Single batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03bfdcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-06-04T02:18:47.904529276Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::hf_compat\u001b[0m\u001b[2m:\u001b[0m num samples received: 1\n",
      "assistant\n",
      "\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Prompt for the model\n",
    "message = [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n",
    "prompt = llm.tokenizer.apply_chat_template(message, tokenize=False)\n",
    "\n",
    "# Generate text\n",
    "response = llm.generate([prompt], sampling_params)\n",
    "\n",
    "# Print the output of the model\n",
    "print(response[0].outputs[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e42ec3",
   "metadata": {},
   "source": [
    "### 2. Multi batch inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f4144a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-06-04T02:33:22.691431827Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfuriosa_generator::scheduler::hf_compat\u001b[0m\u001b[2m:\u001b[0m num samples received: 2\n",
      "Batch 1\n",
      "Question 1: What is the capital of France?\n",
      "Response 1: The capital of France is Paris.\n",
      "====================================================\n",
      "Batch 2\n",
      "Question 2: What is the capital of Germany?\n",
      "Response 2: The capital of Germany is Berlin.\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "messages = [[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
    "            [{\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]]\n",
    "\n",
    "prompts = [llm.tokenizer.apply_chat_template(message, tokenize=False) for message in messages]\n",
    "\n",
    "# Generate text\n",
    "responses = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# Print the output of the model\n",
    "outputs = [responses[i].outputs[0].text.split(\"assistant\\n\\n\")[-1] for i in range(len(responses))]\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"Batch {i+1}\")\n",
    "    print(f\"Question {i + 1}: {messages[i][0]['content']}\")\n",
    "    print(f\"Response {i + 1}: {output}\")\n",
    "    print(\"====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7940430",
   "metadata": {},
   "source": [
    "### 3. Async single batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38613a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant\n",
      "\n",
      "The capital of France is Paris."
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    " \n",
    "async def async_single_batch_inference():\n",
    "    # Prompt for the model\n",
    "    message = [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n",
    "    prompt = llm.tokenizer.apply_chat_template(message, tokenize=False)\n",
    "\n",
    "    # Generate text and print each token at a time\n",
    "    async for output_txt in llm.stream_generate(prompt, sampling_params):\n",
    "        print(output_txt, end=\"\", flush=True)\n",
    "\n",
    "await async_single_batch_inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
